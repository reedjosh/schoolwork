\mysection{Probabilty}
\myssection{Events}
An event is a collection of outcomes of a random experiment \\
$S=\{$collection of all outcomes of the experiment$\}$ \\
$\phi = \{$empty set$\}$ \\
If $A\cap B=\phi$, \\ then $A$ and $B$ are mutually exclusive events\\
DeMorgan's $\bar{(A\cup B)} = (\bar{A}\cap \bar{B})$

\myssection{Axioms and Properties}
\mysssection{Axioms}
I. $P(A)\geq 0$\\
II.\@ $P(S) = 1$ \\
III.\@ If $(A\cap B) = \phi$, \\then $P(A\cup B) = P(A) + P(B)$
$P(A\cup B)=P(A) + P(B) -P(A\cap B)$
$P(\bar{A})=1-P(A)$


\mysssection{Independence}
If $P\{A\cap B\} = P\{A\}P\{B\}$, \\then $A$ and $B$ are independent \\
If $P(A\cap B|C)=P(A|C)P(B|C)$, \\
$A$ and $B$ are {\bf conditionally} independent given event $C$ 

\mysssection{Mutually Exclusivity}
If $P\{A\cap B\} = \phi$, \\then $A$ and $B$ are M.E. 
And, in this case \\
$P(A|B)=P(A)$ and $P(B|A)=P(B)$


\mysssection{Conditional Probability}
$P(A|B)=P(A\cap B)/P(B)$
$P(A\cap B) = P(A|B)P(B) = P(B|A)P(A)$

\mysssection{Bayes' rule}
$P(B|A)=P(A|B)P(B)/P(A)$, \\

%\mysssection{Total Probability}
%If $B_i\cap B_j=\phi$ and $B_1 \cup B_2 \cup ... \cup B_{n-1} \cup B_n=S$, \\ then: $P(A)=\sum_{i=1}^{n}P(A|B_i)P(B_i)$
%
%Bayes for this situation, \\ $P(B_k|A)=P(A|B_k)P(B_k)/P(A)$, \\

\myssection{PDF and CDF}
\mysssection{PDF}
The {\bfseries P}robability {\bfseries D}ensity {\bfseries F}unction is a function that accepts an outcome and 
returns the probability of that outcome occuring. Written as: \\
$p(x)$ and $f_x(x)$\\
\myssection{PMF and CMF}
Are the discrete time versions of the PDF and CDF

\mysssection{CDF}
The {\bfseries C}umulative {\bfseries D}istribution {\bfseries F}unction.
Commonly written as: \\
$P(x)$ and $F_x(x)$ \\
Is the integral of the PDF. 
$F_x(x)=\int f_x(x) dx$








    

